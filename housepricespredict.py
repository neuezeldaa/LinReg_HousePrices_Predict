# -*- coding: utf-8 -*-
"""HousePricesPredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6gvj5U0ZzWTIx3PJizpkm95UOUIclSf

## Решение задачи Регрессии

### Предсказывание цены дома при помощи линейной регресси, Lasso и Ridge

Датасет был взят с https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques

Задача Регрессии - один из типов задачи машинного обучения, целевой переменной (ответом, предсказанием) которой должно стать число. В данном случае - цена квартиры.
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import math
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.linear_model import LassoCV
from sklearn.linear_model import RidgeCV
import warnings

warnings.filterwarnings('ignore')
# some lib

data = pd.read_csv('train.csv', header=0)

data.head(10)

data.shape

"""Проверим пропуски в признаках и выясним какие из признаков можно удалить."""

nuns = data.isnull().sum()
nuns = nuns[nuns>0]
nuns

"""1. Можно удалить MiscFeature, PoolQC, Alley из-за большого количества пропусков/нулей вместо данных.
2. Большинство признаков, связанные с подвалом (basement) не несут никакой полезной информации. Можно оставить только BsmtCond & BsmtQual.
3. Удаляется столбец Id.
"""

data = data.drop(['MiscFeature', 'PoolQC', 'Alley',
           'BsmtFinType1', 'BsmtFinSF1',
           'BsmtFinType2', 'BsmtFinSF2',
           'BsmtUnfSF', 'BsmtExposure', 'Id'], axis=1)

nuns = data.isnull().sum()
nuns = nuns[nuns>0]
nuns_percent = nuns/len(data)
print("Доля пропусков от общего числа значений: ")
print(nuns_percent)

"""Пропущенные категориальные признаки объектов будут заполняться самым часто встречаемым значением данного признака."""

category = data.select_dtypes('object').columns
for column in category:
  top_of_column = data[column].describe()["top"]
  data[column] = data[column].fillna(top_of_column)

"""Пропущенные числовые значения будут заполнены средним значением по признаку."""

numeric = data.select_dtypes('float64').columns
for column in numeric:
  top_of_column = data[column].mean()
  data[column] = data[column].fillna(top_of_column)

nuns = data.isnull().sum()
nuns = nuns[nuns>0]
nuns

"""Пропуски после заполнения отсутствуют."""

data_copy = data.copy()

label_encoding = []
one_hot_encoding = []

for column in category:
  if(data_copy[column].nunique()==2):
    label_encoding.append(column)
  else:
    one_hot_encoding.append(column)

for column in label_encoding:
  data_copy[column] = LabelEncoder().fit_transform(data_copy[column])

data_copy = pd.get_dummies(data_copy, columns=one_hot_encoding, drop_first=True, dtype = int)

data_copy.head()

data_corr = data.drop(labels=category, axis=1)

"""Таблица попарных коэффициентов корреляции Пирсона выглядит следующим образом:"""

fig, ax = plt.subplots(figsize=(25,10))
data_corr = data_corr.corr()
sns.heatmap(data_corr, linewidths=0.001, annot=True)

"""Найдем абсолютные значения коэффициентов корреляции Пирсона с целевой переменнной SalePrice (или насколько сильно тот или иной признак коррелирует с итоговой ценой дома)"""

data_corr = data_corr.abs()

sol = (data_corr.where(np.triu(np.ones(data_corr.shape), k=1).astype(bool))
                  .stack()
                  .sort_values(ascending=False))

features = []
corr = []

for index, value in sol.items():
  if(index[1] == 'SalePrice'):
    features.append(index[0])
    corr.append(value)

data_coef_corr = pd.DataFrame(corr, index = features, columns = ['SalePrice'])
data_coef_corr

fig, ax = plt.subplots(figsize=(20,7))

x = features
y1 = corr

ax.plot(x, y1, color='green', label='Корреляция')
plt.xticks(rotation=25)

ax.grid(True)
plt.xlabel('Признак')
plt.ylabel('Корреляция')
plt.title('Зависимость признаков от целевой переменной')

leg = ax.legend(loc='lower left', frameon=True)

"""Больше всего с итоговым ответом коррелирует признак OverallQual - Общее качество ремонта дома.

Меньше всего коррелирует BsmtHalfBath - Наличие туалетов в подвале.
"""

X = data_copy.drop('SalePrice', axis=1)
y = data['SalePrice']

"""Приведем цены квартир к виду x -> log(1+x) для избавления от разницы в масштабах, а так же избавления от смещения распределения в сторону нормального

В датасете содержится небольшое количество дорогих квартир, из-за чего происходит сильное смещение вправо. Логирование позволяет привести данные к более "нормальному" виду, сделать их более симметричными
"""

plt.hist(np.log1p(data['SalePrice']), color='green', edgecolor='black', bins=int(200/5))

plt.title('Log Saleprice')
plt.xlabel('Цена')
plt.ylabel('Количество квартир')

np.random.seed(13)

"""Разобьем данные в отношении 75:25 на обучающие и тестовые. Также нужно прологарифмировать целевую переменную y."""

X_train, X_test, y_train, y_test = train_test_split(X, np.log1p(y), test_size = 0.25)

"""##Линейная регрессия"""

lin_regr = LinearRegression()
lin_regr.fit(X_train, y_train)

print("lin_reg MSE train: ", mean_squared_error(math.e**(y_train)-1, math.e**(lin_regr.predict(X_train))-1))
print("lin_reg MSE test: ", mean_squared_error(math.e**(y_test)-1, math.e**(lin_regr.predict(X_test))-1))

print("lin_reg MAE train: ", mean_absolute_error(math.e**(y_train)-1, math.e**(lin_regr.predict(X_train))-1))
print("lin_reg MAE test: ", mean_absolute_error(math.e**(y_test)-1, math.e**(lin_regr.predict(X_test))-1))

print("lin_reg r2_score train: ", r2_score(math.e**(y_train)-1, math.e**(lin_regr.predict(X_train))-1))
print("lin_reg r2_score test: ", r2_score(math.e**(y_test)-1, math.e**(lin_regr.predict(X_test))-1))

coef_lin_reg = pd.DataFrame(lin_regr.coef_, index = X.columns, columns=['coef'])
coef_lin_reg.sort_values(by=['coef'])

"""---

##Lasso
"""

lasso = Lasso(alpha=0.001)
lasso.fit(X_train, y_train)

print("lasso MSE train: ", mean_squared_error(math.e**(y_train)-1, math.e**(lasso.predict(X_train))-1))
print("lasso MSE test: ", mean_squared_error(math.e**(y_test)-1, math.e**(lasso.predict(X_test))-1))

print("lasso MAE train: ", mean_absolute_error(math.e**(y_train)-1, math.e**(lasso.predict(X_train))-1))
print("lasso MAE test: ", mean_absolute_error(math.e**(y_test)-1, math.e**(lasso.predict(X_test))-1))

print("lasso r2_score train: ", r2_score(math.e**(y_train)-1, math.e**(lasso.predict(X_train))-1))
print("lasso r2_score test: ", r2_score(math.e**(y_test)-1, math.e**(lasso.predict(X_test))-1))

coef_lasso = pd.DataFrame(lasso.coef_, index = X.columns, columns=['coef_lasso'])
coef_lasso.sort_values(by=['coef_lasso'])

"""---

##Ridge
"""

ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)

print("ridge MSE train: ", mean_squared_error(math.e**(y_train)-1, math.e**(ridge.predict(X_train))-1))
print("ridge MSE test: ", mean_squared_error(math.e**(y_test)-1, math.e**(ridge.predict(X_test))-1))

print("ridge MAE train: ", mean_absolute_error(math.e**(y_train)-1, math.e**(ridge.predict(X_train))-1))
print("ridge MAE test: ", mean_absolute_error(math.e**(y_test)-1, math.e**(ridge.predict(X_test))-1))

print("ridge r2_score train: ", r2_score(math.e**(y_train)-1, math.e**(ridge.predict(X_train))-1))
print("ridge r2_score test: ", r2_score(math.e**(y_test)-1, math.e**(ridge.predict(X_test))-1))

coef_ridge = pd.DataFrame(ridge.coef_, index = X.columns, columns=['coef_ridge'])
coef_ridge.sort_values(by=['coef_ridge'])

"""##Cross-Validation

Кросс-Валидация - это техника, которая помогает понять как обученная модель будет работать на новых данных.
"""

alphas = [0.00001, 0.0001, 0.0003, 0.0005, 0.001, 0.01, 0.5, 1, 2, 6]

lasso_euclid = []
print('Евклидовы нормы весов Lasso')
for i in range(len(alphas)):
  lasso = Lasso(alpha=alphas[i])
  lasso.fit(X_train, y_train)
  lasso_euclid.append(np.linalg.norm(lasso.coef_))
print(lasso_euclid[:5])
print(lasso_euclid[5:])

print("---------------------------------------------------------------")

ridge_euclid = []
print('Евклидовы нормы весов Ridge')
for i in range(len(alphas)):
  ridge = Ridge(alpha=alphas[i])
  ridge.fit(X_train, y_train)
  ridge_euclid.append(np.linalg.norm(ridge.coef_))
print(ridge_euclid[:5])
print(ridge_euclid[5:])

x = alphas
fig, ax = plt.subplots(figsize=(6,4))

y1 = lasso_euclid
y2 = ridge_euclid

ax.plot(x, y1, color='red', label='Lasso')
ax.plot(x,y2, color='blue', label='Ridge')

ax.grid(True)
plt.xlabel('alpha')
plt.ylabel('Euclid norm')
plt.title('Зависимость евклидовой нормы весов от параметров alpha')

leg = ax.legend(loc='upper right', frameon=True)

"""Лучшие гиперпараметры альфа для Lasso & Ridge"""

reg = LassoCV(cv=5, alphas = [0.00001, 0.0001, 0.0003, 0.0005, 0.001, 0.01, 0.5, 1, 2, 6], random_state=0).fit(X_train, y_train)
best_lasso = reg.alpha_
print('Лучший параметр альфа для Lasso: ', best_lasso)

lasso_cv = Lasso(alpha=best_lasso)
lasso_cv.fit(X_train, y_train)
print('lasso cv MSE train: ', mean_squared_error(math.e**(y_train)-1, math.e**(lasso_cv.predict(X_train)-1)))
print('lasso cv MSE test: ', mean_squared_error(math.e**(y_test)-1, math.e**(lasso_cv.predict(X_test)-1)))

coef_lasso_cv = pd.DataFrame(lasso_cv.coef_, index = X.columns, columns=['coef_lasso'])
coef_lasso_cv.sort_values(by=['coef_lasso'])

reg = RidgeCV(cv=5, alphas = [1,2,5,10,20,30]).fit(X_train, y_train)
best_ridge = reg.alpha_
print('Лучший параметр альфа для Ridge: ', best_ridge)

ridge_cv = Ridge(alpha=best_ridge)
ridge_cv.fit(X_train, y_train)
print('ridge cv MSE train: ', mean_squared_error(math.e**(y_train)-1, math.e**(ridge_cv.predict(X_train)-1)))
print('ridge cv MSE test: ', mean_squared_error(math.e**(y_test)-1, math.e**(ridge_cv.predict(X_test)-1)))

coef_ridge_cv = pd.DataFrame(ridge_cv.coef_, index = X.columns, columns=['coef_ridge'])
coef_ridge_cv.sort_values(by=['coef_ridge'])

y1 = coef_lin_reg['coef'].abs()
fig,ax = plt.subplots(figsize = (40,10))

x = X.columns

ax.scatter(x,y1, label = 'Корелляция')
plt.xticks(rotation=90)

ax.grid(True)
plt.xlabel('Признак')
plt.ylabel('Веса')
plt.title('Подобранные веса для каждого из признаков при помощи линейной регрессии')

leg = ax.legend(loc='upper right', frameon=True)

y1 = coef_lasso_cv['coef_lasso'].abs()
fig,ax = plt.subplots(figsize = (40,10))

x = X.columns

ax.scatter(x,y1, label = 'Корелляция')
plt.xticks(rotation=90)

ax.grid(True)
plt.xlabel('Признак')
plt.ylabel('Веса')
plt.title('Подобранные веса для каждого из признаков при помощи Lasso')

leg = ax.legend(loc='upper right', frameon=True)

y1 = coef_ridge_cv['coef_ridge'].abs()
fig,ax = plt.subplots(figsize = (40,10))

x = X.columns

ax.scatter(x,y1, label = 'Корелляция')
plt.xticks(rotation=90)

ax.grid(True)
plt.xlabel('Признак')
plt.ylabel('Веса')
plt.title('Подобранные веса для каждого из признаков при помощи Ridge')

leg = ax.legend(loc='upper right', frameon=True)

print('lin reg MSE test: ', mean_squared_error(math.e**(y_test)-1, math.e**(lin_regr.predict(X_test)-1)))
print('lasso cv MSE test: ', mean_squared_error(math.e**(y_test)-1, math.e**(lasso_cv.predict(X_test)-1)))
print('ridge cv MSE test: ', mean_squared_error(math.e**(y_test)-1, math.e**(ridge_cv.predict(X_test)-1)))

"""На MSE тесте значение ошибки линейной регрессии оказалось меньше, чем при Lasso и Ridge"""